{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Demo\n",
    "\n",
    "Based on this demo: http://nlpforhackers.io/language-models/\n",
    "\n",
    "### Import modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.corpus import reuters, movie_reviews, shakespeare\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose a corpus: reuters, movie_reviews or shakespeare\n",
    "corpus = reuters\n",
    "\n",
    "\n",
    "if corpus==shakespeare:\n",
    "    shakespeare_text = ''.join([''.join(corpus.xml(fileid).itertext()) for fileid in corpus.fileids()])\n",
    "    words = word_tokenize(shakespeare_text)\n",
    "    sents = [word_tokenize(sent) for sent in sent_tokenize(shakespeare_text)]\n",
    "else:    \n",
    "    words = corpus.words()\n",
    "    sents = corpus.sents()\n",
    "\n",
    "# Lowercase everything\n",
    "words = [w.lower() for w in words]\n",
    "sents = [[w.lower() for w in sent] for sent in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram language model\n",
    "\n",
    "In this section, we will construct a language model based on unigrams (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.\n",
    "# \"words\" is a list containing all the words in the corpus (including multiples of the same word)\n",
    "# Make a Counter from the list of words and call it \"unigram_counts\" (remember, this is easy to do!)\n",
    "# Get the total number of words and assign it to \"total_count\"\n",
    "\n",
    "##### YOUR CODE STARTS HERE #####\n",
    "\n",
    "\n",
    "\n",
    "##### YOUR CODE ENDS HERE #####\n",
    "\n",
    "print \"Total number of words in corpus: \", total_count\n",
    "\n",
    "# Print 10 most common words\n",
    "print \"\"\n",
    "for (word, count) in unigram_counts.most_common(n=10):\n",
    "    print word, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.\n",
    "# Compute the probability of each word (unigram) from the counts.\n",
    "# Create a Counter called \"unigram_probs\" that holds the probability of each word.\n",
    "# Hint: use the variables unigram_counts and total_count, that you just created.\n",
    "# Hint: remember about integer division!\n",
    "\n",
    "##### YOUR CODE STARTS HERE #####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### YOUR CODE ENDS HERE #####\n",
    "\n",
    "# Check the probabilities add up to 1\n",
    "print \"Probabilities sum to: \", sum(unigram_probs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the probability of word \"the\", then try some other words.\n",
    "print unigram_probs['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100 words of language using unigram model\n",
    "\n",
    "text = [] # will be a list of generated words\n",
    "\n",
    "for _ in range(100):\n",
    "    r = random.random() # get a random value in [0,1]\n",
    "    \n",
    "    # Find the word whose interval contains r\n",
    "    accumulator = .0\n",
    "    for word, freq in unigram_probs.iteritems():\n",
    "        accumulator += freq\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3. \n",
    "# Calculate the probability of the text we generated. Note the variable \"text\" is a list of words.\n",
    "# Hint: we want to take each of the words in the list \"text\", \n",
    "# look up their probability in the dictionary unigram_probs, and multiply all the probabilities together.\n",
    "\n",
    "##### YOUR CODE STARTS HERE #####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### YOUR CODE ENDS HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count how often each bigram occurs.\n",
    "\n",
    "# bigram_counts maps w1 to a dictionary mapping w2 to the count for (w1, w2)\n",
    "bigram_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "for sentence in sents:\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "        bigram_counts[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print how often the bigram \"of the\" occurs. Try some other words following \"of\".\n",
    "print bigram_counts['of']['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the bigram counts to bigram probabilities\n",
    "bigram_probs = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for w1 in bigram_counts:\n",
    "    total_count = float(sum(bigram_counts[w1].values()))\n",
    "    bigram_probs[w1] = Counter({w2: c/total_count for w2,c in bigram_counts[w1].iteritems()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the probability that 'the' follows 'of'\n",
    "print bigram_probs['of']['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top ten tokens most likely to follow 'fair', along with their probabilities\n",
    "prob_dist = bigram_probs['fair']\n",
    "for word,prob in prob_dist.most_common(10):\n",
    "    print word,prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with bigram model\n",
    "\n",
    "text = [None]\n",
    "sentence_finished = False\n",
    "\n",
    "# Generate words until a None is generated\n",
    "while not sentence_finished:\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "    latest_token = text[-1]\n",
    "    prob_dist = bigram_probs[latest_token] # prob dist of what token comes next\n",
    "        \n",
    "    for word,p in prob_dist.iteritems():\n",
    "        accumulator += p \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-1] == None:\n",
    "        sentence_finished = True\n",
    "\n",
    "print ' '.join([t for t in text if t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probability of the text we just generated\n",
    "# This is just the probability of each of the bigrams, multiplied.\n",
    "\n",
    "text_prob = 1.0\n",
    "text_len = len(text)\n",
    "\n",
    "for idx in range(text_len-1):\n",
    "    w1 = text[idx]\n",
    "    w2 = text[idx+1]\n",
    "    text_prob *= bigram_probs[w1][w2]\n",
    "    \n",
    "print text_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count how often each trigram occurs.\n",
    "\n",
    "# trigram_counts maps (w1, w2) to a dictionary mapping w3 to the count for (w1, w2, w3)\n",
    "trigram_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "for sentence in sents:\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        trigram_counts[(w1, w2)][w3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print how often the trigram \"I am not\" occurs. Try some other trigrams.\n",
    "print trigram_counts[('i', 'am')]['not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the trigram counts to trigram probabilities\n",
    "trigram_probs = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for w1_w2 in trigram_counts:\n",
    "    total_count = float(sum(trigram_counts[w1_w2].values()))\n",
    "    trigram_probs[w1_w2] = Counter({w3: c/total_count for w3,c in trigram_counts[w1_w2].iteritems()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the probability that 'not' follows 'i am'. Try some other combinations.\n",
    "print trigram_probs[('i', 'am')]['not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top ten tokens most likely to follow 'i am', along with their probabilities\n",
    "prob_dist = trigram_probs[('i', 'am')]\n",
    "for word,prob in prob_dist.most_common(10):\n",
    "    print word,prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with trigram model\n",
    "\n",
    "text = [None, None]\n",
    "\n",
    "sentence_finished = False\n",
    "\n",
    "# Generate words until two consecutive Nones are generated\n",
    "while not sentence_finished:\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "    latest_bigram = tuple(text[-2:])\n",
    "    prob_dist = trigram_probs[latest_bigram] # prob dist of what token comes next\n",
    "    \n",
    "    for word,p in prob_dist.iteritems():\n",
    "        accumulator += p \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "print ' '.join([t for t in text if t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of the text we generated\n",
    "# This is just the product of the probability of each trigram\n",
    "\n",
    "text_prob = 1.0\n",
    "text_len = len(text)\n",
    "\n",
    "for idx in range(text_len-2):\n",
    "    w1 = text[idx]\n",
    "    w2 = text[idx+1]\n",
    "    w3 = text[idx+2]\n",
    "    text_prob *= trigram_probs[(w1,w2)][w3]\n",
    "    \n",
    "print text_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
